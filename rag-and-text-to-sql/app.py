# Required imports
import os
import uuid
import asyncio
import tempfile
import time
import nest_asyncio
from dotenv import load_dotenv
import streamlit as st
from streamlit_pdf_viewer import pdf_viewer

# Import necessary components from llama_index
from llama_index.core import Settings
from llama_index.llms.ollama import Ollama
# from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.embeddings.fastembed import FastEmbedEmbedding
from tools import setup_document_tool, setup_sql_tool
from workflow import RouterOutputAgentWorkflow

# Configure Opik for Llama Index
import opik
opik.configure(use_local=False)

from llama_index.core.callbacks import CallbackManager
from opik.integrations.llama_index import LlamaIndexCallbackHandler

opik_callback_handler = LlamaIndexCallbackHandler()
Settings.callback_manager = CallbackManager([opik_callback_handler])

# Load environment variables
load_dotenv()

# Apply nest_asyncio to allow running asyncio in Streamlit
nest_asyncio.apply()

# Set page configuration
st.set_page_config(
    page_title="RAG & SQL Fusion üîó",
    page_icon="üèôÔ∏è",
    layout="wide",
)

# Initialize session state
if "file_uploaded" not in st.session_state:
    st.session_state.file_uploaded = False

if "messages" not in st.session_state:
    st.session_state.messages = []

if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}

if "workflow" not in st.session_state:
    st.session_state.workflow = None

if "workflow_needs_update" not in st.session_state:
    st.session_state.workflow_needs_update = False

if "llm_initialized" not in st.session_state:
    st.session_state.llm_initialized = False


#####################################
# Helper Functions
#####################################
def reset_chat():
    """Reset the chat history and clear context"""
    # Clear messages immediately
    if "messages" in st.session_state:
        st.session_state.messages = []

    if "workflow" in st.session_state and st.session_state.workflow:
        st.session_state.workflow = None
        st.session_state.workflow_needs_update = True


def display_pdf(file_path):
    """Display PDF in the Streamlit app using a dedicated PDF viewer component"""
    try:
        # Use the pdf_viewer component with recommended settings
        pdf_viewer(
            file_path,
            width="100%",  # Use percentage for responsive width
            height=600,  # Fixed height in pixels
            render_text=True,  # Enable text layer for copy-paste
            pages_vertical_spacing=5,  # Add more space between pages
            annotation_outline_size=2,  # Make any annotations more visible
            pages_to_render=[1, 2, 3],  # Render only the first 3 pages
        )

        # Provide download option even when viewer works
        with open(file_path, "rb") as pdf_file:
            pdf_bytes = pdf_file.read()
            st.download_button(
                label="üì• Download PDF",
                data=pdf_bytes,
                file_name=os.path.basename(file_path),
                mime="application/pdf",
            )

    except Exception as e:
        st.error(f"Error displaying PDF: {str(e)}")
        st.warning("To view the PDF, you can download it using the button below")

        # Fallback to download only
        try:
            with open(file_path, "rb") as pdf_file:
                st.download_button(
                    label="üì• Download PDF",
                    data=pdf_file.read(),
                    file_name=os.path.basename(file_path),
                    mime="application/pdf",
                )
        except Exception as download_error:
            st.error(f"Unable to provide PDF download option: {str(download_error)}")


@st.cache_resource
def initialize_model():
    """Initialize models for LLM and embedding"""
    try:
        # Initialize models for LLM and embedding
        llm = Ollama(model="mistral", request_timeout=120.0)
        # embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
        embed_model = FastEmbedEmbedding(model_name="BAAI/bge-small-en-v1.5")
        return llm, embed_model
    except Exception as e:
        st.error(f"Error initializing models: {str(e)}")
        return None


def handle_file_upload(uploaded_files):
    """Function to handle multiple file uploads with temporary directory"""
    try:
        # Create a temporary directory if it doesn't exist yet
        if not hasattr(st.session_state, "temp_dir") or not os.path.exists(
            st.session_state.temp_dir
        ):
            st.session_state.temp_dir = tempfile.mkdtemp()

        # Track uploaded files
        if "uploaded_files" not in st.session_state:
            st.session_state.uploaded_files = []

        # Process each uploaded file
        file_paths = []
        for uploaded_file in uploaded_files:
            # Save file to temporary location
            temp_file_path = os.path.join(st.session_state.temp_dir, uploaded_file.name)

            # Write the file
            with open(temp_file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            # Add to list of uploaded files
            st.session_state.uploaded_files.append(
                {"name": uploaded_file.name, "path": temp_file_path}
            )
            file_paths.append(temp_file_path)

        st.session_state.file_uploaded = True
        st.session_state.current_pdf = (
            file_paths[0] if file_paths else None
        )  # Set first file as current for preview
        st.session_state.workflow_needs_update = True  # Mark workflow for update

        return file_paths
    except Exception as e:
        st.error(f"Error uploading files: {str(e)}")
        return None


def initialize_workflow(tools):
    """Initialize workflow with the given tools"""
    try:
        workflow = RouterOutputAgentWorkflow(tools=tools, verbose=False, timeout=120)
        st.session_state.workflow = workflow
        return workflow
    except Exception as e:
        st.error(f"Error initializing workflow: {str(e)}")
        return None


async def process_query(query, workflow):
    """Function to process a query using the workflow"""
    try:
        with st.spinner("Processing your query..."):
            result = await workflow.run(message=query)
        return result
    except Exception as e:
        print(f"Error in process_query: {str(e)}")
        return "I'm sorry, I encountered an error while processing your request. Please try a different question."


#####################################
# Main Streamlit app
#####################################
def main():
    # Sidebar configuration
    with st.sidebar:
        st.header("Configuration")

        # Model information
        st.info("This app uses Ollama for local inference.")

        # LLM initialization status
        if not st.session_state.llm_initialized:
            with st.spinner("Initializing models..."):
                llm, embed_model = initialize_model()
                if llm:
                    Settings.llm = llm
                    Settings.embed_model = embed_model
                    st.session_state.llm_initialized = True
                    st.success("Models initialized!")
                else:
                    st.error("Failed to initialize models.")
        else:
            st.success("Models: Ready ‚úì")

        # File upload section in the sidebar
        st.subheader("Documents")
        uploaded_files = st.file_uploader(
            "Choose PDF files", type=["pdf"], accept_multiple_files=True
        )

        if uploaded_files:
            if st.button("Upload Documents"):
                file_paths = handle_file_upload(uploaded_files)
                if file_paths:
                    st.success(f"{len(file_paths)} document(s) uploaded")

                    # Display list of uploaded documents
                    st.write("Uploaded documents:")
                    for i, file in enumerate(st.session_state.uploaded_files):
                        st.write(f"- {file['name']}")

        # Document status indicator
        if st.session_state.file_uploaded:
            st.success("Documents: ‚úì")
        else:
            st.warning("Documents: ‚úó")

    # Chat title and reset button in the same row
    chat_header_col1, chat_header_col2 = st.columns([5, 1])
    with chat_header_col1:
        st.title("RAG & SQL Fusion üîó")
    with chat_header_col2:
        st.button("Reset Chat ‚Ü∫", on_click=reset_chat)

    # Continue only if LLM is initialized
    if st.session_state.llm_initialized:
        # Initialize tools with first tool as SQL tool
        tools = [setup_sql_tool()]

        if st.session_state.file_uploaded:
            file_key = f"{st.session_state.id}-documents"

            if file_key not in st.session_state.file_cache:
                with st.spinner("Processing documents..."):
                    # Use the uploaded documents to create a document tool
                    document_tool = setup_document_tool(
                        file_dir=st.session_state.temp_dir
                    )
                    st.session_state.file_cache[file_key] = document_tool
                st.success("Documents processed successfully!")

            tools.append(st.session_state.file_cache[file_key])

        # Initialize or update workflow with tools
        if not st.session_state.workflow or st.session_state.workflow_needs_update:
            workflow = initialize_workflow(tools)
            st.session_state.workflow_needs_update = False
        else:
            workflow = st.session_state.workflow

        # Display PDF preview if files are uploaded
        if (
            st.session_state.file_uploaded
            and hasattr(st.session_state, "uploaded_files")
            and st.session_state.uploaded_files
        ):
            # Document selector
            doc_names = [doc["name"] for doc in st.session_state.uploaded_files]
            selected_doc = st.selectbox("Select document to preview:", doc_names)

            # Find the selected document path
            selected_path = next(
                (
                    doc["path"]
                    for doc in st.session_state.uploaded_files
                    if doc["name"] == selected_doc
                ),
                None,
            )

            if (
                selected_path
                and os.path.exists(selected_path)
                and os.path.getsize(selected_path) > 0
            ):
                with st.expander("Document Preview", expanded=False):
                    display_pdf(selected_path)
            else:
                st.warning("Selected document not available for preview.")
        else:
            st.info("Upload documents to enable preview.")

        # Chat interface
        if workflow:
            # Create a container for messages with fixed height
            chat_container = st.container(height=500, border=True)

            with chat_container:
                for message in st.session_state.messages:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])

            # User input at the bottom
            query = st.chat_input("Ask your question...")

            # Process query if submitted
            if query:
                st.session_state.messages.append({"role": "user", "content": query})

                # Update the display inside the container to include all messages
                with chat_container:
                    with st.chat_message("user"):
                        st.markdown(query)

                    # Process and show assistant response inside the container
                    with st.chat_message("assistant"):
                        message_placeholder = st.empty()
                        message_placeholder.markdown("Thinking...")

                        # Process the query
                        response = asyncio.run(process_query(query, workflow))

                        # Simulate streaming effect for better UX
                        displayed_response = ""
                        words = response.split()
                        for _, word in enumerate(words):
                            displayed_response += word + " "
                            message_placeholder.markdown(displayed_response + "‚ñå")
                            # Adjust delay for natural reading pace
                            time.sleep(0.01)  # Faster typing for better UX

                        # Final display without cursor
                        message_placeholder.markdown(displayed_response)

                    # Add assistant response to chat history
                    st.session_state.messages.append(
                        {"role": "assistant", "content": displayed_response.strip()}
                    )
            else:
                # Display existing messages in the container
                with chat_container:
                    for message in st.session_state.messages:
                        with st.chat_message(message["role"]):
                            st.markdown(message["content"])
    else:
        # Application information if LLM not initialized
        st.error("Ollama could not be initialized. Please make sure it is running.")
        st.markdown(
            """
            ### Ollama Setup
            1. Make sure Ollama is installed and running on your machine
            2. Ensure the 'mistral' model is downloaded via `ollama pull mistral`
            3. Restart this application
            
            Once Ollama is running, you'll be able to:
            1. Query information about U.S. cities
            2. Upload and analyze PDF documents
            """
        )


if __name__ == "__main__":
    # Clean up any temporary directories on exit
    if hasattr(st.session_state, "temp_dir") and os.path.exists(
        st.session_state.temp_dir
    ):
        try:
            os.rmdir(st.session_state.temp_dir)
        except:
            pass

    # Run the main Streamlit app
    main()
