{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-openai\n",
    "!pip install llama-index-vector-stores-qdrant\n",
    "!pip install qdrant-client\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Insert_your_OpenAI_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SQLDatabase, Settings \n",
    "from llama_index.llms.openai import OpenAI\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "\n",
    "Settings.llm = OpenAI(\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\", future=True)\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"state\", String(16), nullable=False),\n",
    ")\n",
    "\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "\n",
    "rows = [\n",
    "    {\"city_name\": \"New York City\", \"population\": 8336000, \"state\": \"New York\"},\n",
    "    {\"city_name\": \"Los Angeles\", \"population\": 3822000, \"state\": \"California\"},\n",
    "    {\"city_name\": \"Chicago\", \"population\": 2665000, \"state\": \"Illinois\"},\n",
    "    {\"city_name\": \"Houston\", \"population\": 2303000, \"state\": \"Texas\"},\n",
    "    {\"city_name\": \"Miami\", \"population\": 449514, \"state\": \"Florida\"},\n",
    "    {\"city_name\": \"Seattle\", \"population\": 749256, \"state\": \"Washington\"},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(city_stats_table).values(**row)\n",
    "    with engine.begin() as connection:\n",
    "        cursor = connection.execute(stmt)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    cursor = connection.exec_driver_sql(\"SELECT * FROM city_stats\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Query Engine Based on SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "sql_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    tables=[\"city_stats\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating index locally on Using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex imports (updated for newer versions)\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, Settings\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.llms.llm import ToolSelection, LLM\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "from llama_index.core.workflow.context import Context\n",
    "from llama_index.core.base.response.schema import Response\n",
    "\n",
    "# Import Qdrant vector store from the correct package\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# Import OpenAI LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Qdrant imports\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qdrant_models\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from qdrant_client import QdrantClient\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index qdrant-client torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the embedding model\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"Snowflake/snowflake-arctic-embed-m\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load PDF documents from directory\n",
    "pdf_dir = \"C:/Users/prita/Documents/DailyDose\"\n",
    "# Change this to your PDF directory path\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=pdf_dir,\n",
    "    required_exts=[\".pdf\"],\n",
    "    recursive=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load_data()\n",
    "print(f\"Loaded {len(documents)} PDF documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Qdrant vector store (in-memory for simplicity)\n",
    "client = QdrantClient(\":memory:\")\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"pdf_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    vector_store=vector_store\n",
    ")\n",
    "print(\"Documents indexed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,\n",
    "    description=(\n",
    "        \"Useful for translating a natural language query into a SQL query over\"\n",
    "        \" a table containing: city_stats, containing the population/state of\"\n",
    "        \" each city located in the USA.\"\n",
    "    ),\n",
    "    name=\"sql_tool\"\n",
    ")\n",
    "\n",
    "cities = [\"New York City\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Miami\", \"Seattle\"]\n",
    "rag_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=rag_query_engine,\n",
    "    description=(\n",
    "        f\"Useful for answering semantic questions about certain cities in the US.\"\n",
    "    ),\n",
    "    name=\"llama_cloud_tool\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Assuming you already have the tools defined as in your example:\n",
    "# sql_tool and rag_tool\n",
    "\n",
    "def create_agent(tools: List[QueryEngineTool], model_name: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"Create a simple ReAct agent with the provided tools.\"\"\"\n",
    "    # Initialize the LLM\n",
    "    llm = OpenAI(model=model_name)\n",
    "    \n",
    "    # Create the agent with the tools\n",
    "    agent = ReActAgent.from_tools(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        verbose=True,  # Show the agent's thought process\n",
    "    )\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(tools=[sql_tool, rag_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query: str):\n",
    "    \"\"\"Process a query using our agent and display the result.\"\"\"\n",
    "    try:\n",
    "        response = agent.query(query)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"What is the population of Miami?\"\n",
    "print(f\"Query: {sql_query}\")\n",
    "print(f\"Response: {process_query(sql_query)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter your question: \")\n",
    "print(f\"Response: {process_query(query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing a streamlit UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rag_sql_app_upload.py\n",
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "import tempfile\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Set page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Magic of RAG and Text to SQL\",\n",
    "    layout=\"centered\"\n",
    ")\n",
    "\n",
    "# Title and description\n",
    "st.title(\"Magic of RAG and Text to SQL\")\n",
    "st.write(\"Upload PDFs and ask questions using RAG or SQL\")\n",
    "\n",
    "# Initialize session state variables\n",
    "if 'index' not in st.session_state:\n",
    "    st.session_state.index = None\n",
    "if 'processed' not in st.session_state:\n",
    "    st.session_state.processed = False\n",
    "\n",
    "def process_documents(directory):\n",
    "    try:\n",
    "        # Set up embedding model\n",
    "        embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Set up OpenAI LLM\n",
    "        llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        \n",
    "        # Create node parser\n",
    "        node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "        \n",
    "        # Load documents\n",
    "        documents = SimpleDirectoryReader(directory).load_data()\n",
    "        \n",
    "        # Create index with components directly\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents,\n",
    "            llm=llm,\n",
    "            embed_model=embed_model,\n",
    "            transformations=[node_parser]\n",
    "        )\n",
    "        \n",
    "        return index\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing documents: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Option to select an existing directory or upload files\n",
    "option = st.radio(\"Choose an option:\", [\"Select directory\", \"Upload PDF files\"])\n",
    "\n",
    "if option == \"Select directory\":\n",
    "    directory = st.text_input(\"Enter the directory path containing PDFs:\")\n",
    "    \n",
    "    if directory and os.path.isdir(directory):\n",
    "        st.success(f\"Directory selected: {directory}\")\n",
    "        \n",
    "        # Process PDFs button\n",
    "        if st.button(\"Process PDFs\"):\n",
    "            with st.spinner(\"Processing PDFs...\"):\n",
    "                st.session_state.index = process_documents(directory)\n",
    "                if st.session_state.index is not None:\n",
    "                    st.session_state.processed = True\n",
    "                    st.success(\"PDFs processed successfully!\")\n",
    "else:\n",
    "    uploaded_files = st.file_uploader(\"Upload PDF files\", accept_multiple_files=True, type=[\"pdf\"])\n",
    "    \n",
    "    if uploaded_files:\n",
    "        if st.button(\"Process Uploaded PDFs\"):\n",
    "            # Create a temporary directory to store the uploaded files\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                # Save uploaded files to the temporary directory\n",
    "                for uploaded_file in uploaded_files:\n",
    "                    file_path = os.path.join(temp_dir, uploaded_file.name)\n",
    "                    with open(file_path, \"wb\") as f:\n",
    "                        f.write(uploaded_file.getbuffer())\n",
    "                \n",
    "                with st.spinner(\"Processing PDFs...\"):\n",
    "                    st.session_state.index = process_documents(temp_dir)\n",
    "                    if st.session_state.index is not None:\n",
    "                        st.session_state.processed = True\n",
    "                        st.success(\"PDFs processed successfully!\")\n",
    "\n",
    "# Show chatbot interface if PDFs are processed\n",
    "if st.session_state.processed and st.session_state.index is not None:\n",
    "    # Query type selection\n",
    "    query_type = st.radio(\"Select query type:\", [\"RAG\", \"SQL\"], horizontal=True)\n",
    "    \n",
    "    # Query input\n",
    "    query = st.text_input(\"Your question:\")\n",
    "    \n",
    "    if query:\n",
    "        with st.spinner(\"Generating answer...\"):\n",
    "            try:\n",
    "                if query_type == \"RAG\":\n",
    "                    # RAG query\n",
    "                    query_engine = st.session_state.index.as_query_engine()\n",
    "                    response = query_engine.query(query)\n",
    "                    st.write(\"### Answer:\")\n",
    "                    st.write(response.response)\n",
    "                else:\n",
    "                    # SQL query\n",
    "                    # Replace with your SQL implementation\n",
    "                    st.write(\"### Answer:\")\n",
    "                    st.write(\"SQL query execution feature is coming soon.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error generating answer: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run rag_sql_app_upload.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
